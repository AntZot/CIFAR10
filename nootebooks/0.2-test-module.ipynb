{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CIFAR_CNN.data.data_loader import CIFAR10DataModule\n",
    "from CIFAR_CNN.models.cnn import CIFAR10Model\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "import wandb\n",
    "from CIFAR_CNN.models.train_model import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "234"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.seed_everything(234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 4060 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mantzot\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20231102_024902-ncqifdn9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/antzot/lightning_logs/runs/ncqifdn9' target=\"_blank\">swept-microwave-66</a></strong> to <a href='https://wandb.ai/antzot/lightning_logs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/antzot/lightning_logs' target=\"_blank\">https://wandb.ai/antzot/lightning_logs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/antzot/lightning_logs/runs/ncqifdn9' target=\"_blank\">https://wandb.ai/antzot/lightning_logs/runs/ncqifdn9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainerFn.FITTING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type       | Params\n",
      "---------------------------------------------\n",
      "0 | cnn_relu_seq  | Sequential | 3.6 K \n",
      "1 | lin_layer_seq | Sequential | 93.4 K\n",
      "---------------------------------------------\n",
      "97.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "97.0 K    Total params\n",
      "0.388     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/dl_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/root/miniconda3/envs/dl_env/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/dl_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 176/176 [00:09<00:00, 18.44it/s, v_num=fdn9, train_rocauc=0.471, val_rocauc=0.496, val_loss=2.300]{'loss': tensor(2.3088, device='cuda:0')}\n",
      "(tensor([[[[ 0.0196, -0.0118, -0.0118,  ...,  0.1137,  0.1294,  0.1686],\n",
      "          [ 0.0353,  0.0039, -0.0510,  ...,  0.1216,  0.1373,  0.1608],\n",
      "          [ 0.0196,  0.0196, -0.0196,  ...,  0.1216,  0.1294,  0.1608],\n",
      "          ...,\n",
      "          [-0.1059, -0.1216, -0.1216,  ..., -0.1059, -0.1529, -0.0980],\n",
      "          [-0.1216, -0.1765, -0.2078,  ..., -0.1137, -0.0980, -0.1137],\n",
      "          [-0.1843, -0.1686, -0.1922,  ..., -0.1373, -0.1216, -0.0980]],\n",
      "\n",
      "         [[ 0.3647,  0.3255,  0.3255,  ...,  0.3725,  0.3647,  0.3961],\n",
      "          [ 0.3725,  0.3412,  0.2863,  ...,  0.3725,  0.3725,  0.3961],\n",
      "          [ 0.3647,  0.3569,  0.3176,  ...,  0.3725,  0.3725,  0.3961],\n",
      "          ...,\n",
      "          [ 0.1451,  0.1451,  0.1529,  ...,  0.1216,  0.0745,  0.1216],\n",
      "          [ 0.1216,  0.0902,  0.0745,  ...,  0.1059,  0.1137,  0.0902],\n",
      "          [ 0.0667,  0.0980,  0.0824,  ...,  0.0824,  0.0902,  0.1059]],\n",
      "\n",
      "         [[ 0.4902,  0.4510,  0.4431,  ...,  0.4824,  0.4667,  0.4980],\n",
      "          [ 0.5059,  0.4667,  0.4196,  ...,  0.4902,  0.4745,  0.4902],\n",
      "          [ 0.4980,  0.4902,  0.4510,  ...,  0.4902,  0.4824,  0.4980],\n",
      "          ...,\n",
      "          [ 0.3412,  0.3255,  0.3333,  ...,  0.3020,  0.2549,  0.3020],\n",
      "          [ 0.3098,  0.2627,  0.2471,  ...,  0.2784,  0.2941,  0.2706],\n",
      "          [ 0.2549,  0.2706,  0.2549,  ...,  0.2549,  0.2627,  0.2863]]],\n",
      "\n",
      "\n",
      "        [[[-0.5294, -0.5373, -0.5294,  ..., -0.4353, -0.4824, -0.3569],\n",
      "          [-0.5137, -0.5373, -0.5529,  ..., -0.4431, -0.4824, -0.3020],\n",
      "          [-0.4745, -0.5373, -0.5373,  ..., -0.4588, -0.5137, -0.3176],\n",
      "          ...,\n",
      "          [-0.5294, -0.6000, -0.5922,  ..., -0.5922, -0.6314, -0.3882],\n",
      "          [-0.5686, -0.6157, -0.5922,  ..., -0.6000, -0.6314, -0.3725],\n",
      "          [-0.4510, -0.4667, -0.4510,  ..., -0.4667, -0.5059, -0.3098]],\n",
      "\n",
      "         [[-0.2235, -0.1922, -0.2000,  ..., -0.1373, -0.1686, -0.0667],\n",
      "          [-0.1059, -0.0353, -0.0510,  ...,  0.0667,  0.0353,  0.1373],\n",
      "          [-0.0980, -0.0196, -0.0431,  ...,  0.0902,  0.0588,  0.1451],\n",
      "          ...,\n",
      "          [-0.4039, -0.3961, -0.3882,  ..., -0.4745, -0.4902, -0.2863],\n",
      "          [-0.4196, -0.4196, -0.3961,  ..., -0.4980, -0.5216, -0.2941],\n",
      "          [-0.3255, -0.3098, -0.3176,  ..., -0.3961, -0.4353, -0.2549]],\n",
      "\n",
      "         [[-0.1608, -0.0902, -0.1059,  ...,  0.0275, -0.0039,  0.0353],\n",
      "          [-0.0275,  0.1137,  0.1373,  ...,  0.3255,  0.2863,  0.3020],\n",
      "          [ 0.0118,  0.1686,  0.2078,  ...,  0.3020,  0.2549,  0.2863],\n",
      "          ...,\n",
      "          [-0.4745, -0.5137, -0.5216,  ..., -0.6471, -0.6471, -0.4039],\n",
      "          [-0.4902, -0.5294, -0.5216,  ..., -0.6941, -0.7020, -0.4275],\n",
      "          [-0.3882, -0.4118, -0.4275,  ..., -0.5608, -0.6000, -0.3804]]],\n",
      "\n",
      "\n",
      "        [[[-0.0824, -0.1059, -0.1059,  ...,  0.1451,  0.0902,  0.0667],\n",
      "          [-0.1686, -0.2549, -0.3255,  ...,  0.0902,  0.0118, -0.0431],\n",
      "          [-0.3255, -0.4588, -0.4902,  ...,  0.0588,  0.1059, -0.0196],\n",
      "          ...,\n",
      "          [ 0.3176,  0.2471,  0.2235,  ..., -0.3804, -0.3255, -0.2863],\n",
      "          [ 0.3882,  0.3725,  0.3176,  ..., -0.2627, -0.1922, -0.1294],\n",
      "          [ 0.3647,  0.4667,  0.4275,  ..., -0.2235, -0.0745,  0.0510]],\n",
      "\n",
      "         [[-0.2627, -0.2784, -0.2706,  ..., -0.0588, -0.1059, -0.1373],\n",
      "          [-0.3176, -0.3490, -0.3882,  ..., -0.0824, -0.1686, -0.2157],\n",
      "          [-0.4510, -0.4980, -0.4902,  ..., -0.1216, -0.0745, -0.2000],\n",
      "          ...,\n",
      "          [ 0.1843,  0.1059,  0.0431,  ..., -0.4588, -0.4588, -0.4588],\n",
      "          [ 0.2784,  0.2471,  0.1451,  ..., -0.3647, -0.3176, -0.2941],\n",
      "          [ 0.2941,  0.3725,  0.2863,  ..., -0.3725, -0.2392, -0.1373]],\n",
      "\n",
      "         [[-0.3176, -0.3569, -0.3412,  ..., -0.1294, -0.1843, -0.2078],\n",
      "          [-0.3804, -0.4118, -0.4353,  ..., -0.1686, -0.2471, -0.3020],\n",
      "          [-0.4824, -0.5216, -0.4902,  ..., -0.2078, -0.1608, -0.2784],\n",
      "          ...,\n",
      "          [ 0.0824, -0.0039, -0.0745,  ..., -0.4667, -0.4745, -0.4667],\n",
      "          [ 0.1686,  0.1294,  0.0275,  ..., -0.4196, -0.4039, -0.3804],\n",
      "          [ 0.2078,  0.2863,  0.2078,  ..., -0.4275, -0.3176, -0.2314]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.6627,  0.6000,  0.5843,  ...,  0.6863,  0.6549,  0.6314],\n",
      "          [ 0.6549,  0.6392,  0.6706,  ...,  0.6863,  0.6941,  0.6706],\n",
      "          [ 0.6706,  0.6078,  0.6078,  ...,  0.6000,  0.6235,  0.5922],\n",
      "          ...,\n",
      "          [ 0.5529,  0.6235,  0.7020,  ...,  0.6627,  0.6627,  0.5608],\n",
      "          [ 0.6784,  0.7020,  0.6706,  ...,  0.6784,  0.5608,  0.4118],\n",
      "          [ 0.7255,  0.6863,  0.5686,  ...,  0.5922,  0.5216,  0.4824]],\n",
      "\n",
      "         [[ 0.6235,  0.5608,  0.5451,  ...,  0.6000,  0.5765,  0.5686],\n",
      "          [ 0.6627,  0.6392,  0.6627,  ...,  0.6235,  0.6078,  0.5608],\n",
      "          [ 0.7020,  0.6235,  0.6157,  ...,  0.5451,  0.5373,  0.4667],\n",
      "          ...,\n",
      "          [ 0.5451,  0.6078,  0.7176,  ...,  0.6157,  0.6314,  0.5373],\n",
      "          [ 0.6941,  0.7412,  0.7020,  ...,  0.5686,  0.5059,  0.3882],\n",
      "          [ 0.7020,  0.6941,  0.5608,  ...,  0.4667,  0.4510,  0.4588]],\n",
      "\n",
      "         [[ 0.4431,  0.4039,  0.3961,  ...,  0.4275,  0.3804,  0.3490],\n",
      "          [ 0.4667,  0.4980,  0.5765,  ...,  0.4431,  0.4039,  0.3255],\n",
      "          [ 0.4980,  0.4902,  0.5529,  ...,  0.3647,  0.3412,  0.2392],\n",
      "          ...,\n",
      "          [ 0.3882,  0.4745,  0.5922,  ...,  0.5765,  0.6000,  0.4667],\n",
      "          [ 0.5922,  0.6471,  0.5765,  ...,  0.4902,  0.4510,  0.3176],\n",
      "          [ 0.5922,  0.5843,  0.4039,  ...,  0.3490,  0.3647,  0.3725]]],\n",
      "\n",
      "\n",
      "        [[[-0.5216, -0.4588, -0.4118,  ..., -0.3176, -0.3333, -0.3569],\n",
      "          [-0.4824, -0.4588, -0.4196,  ..., -0.3176, -0.3255, -0.3176],\n",
      "          [-0.4667, -0.4588, -0.4275,  ..., -0.3255, -0.3176, -0.4196],\n",
      "          ...,\n",
      "          [-0.4196, -0.3961, -0.3412,  ..., -0.2941, -0.3961, -0.3804],\n",
      "          [-0.4118, -0.4118, -0.3569,  ..., -0.2784, -0.2706, -0.2627],\n",
      "          [-0.4431, -0.4275, -0.3804,  ..., -0.2235, -0.2392, -0.2627]],\n",
      "\n",
      "         [[-0.6549, -0.6000, -0.5686,  ..., -0.4902, -0.4902, -0.5059],\n",
      "          [-0.6314, -0.6078, -0.5765,  ..., -0.4745, -0.4824, -0.4980],\n",
      "          [-0.6157, -0.6078, -0.5765,  ..., -0.4824, -0.4902, -0.5686],\n",
      "          ...,\n",
      "          [-0.5451, -0.5373, -0.4902,  ..., -0.3725, -0.4745, -0.4667],\n",
      "          [-0.5451, -0.5451, -0.4980,  ..., -0.3647, -0.3647, -0.3569],\n",
      "          [-0.5686, -0.5529, -0.5137,  ..., -0.3098, -0.3255, -0.3490]],\n",
      "\n",
      "         [[-0.7098, -0.6627, -0.6392,  ..., -0.5373, -0.5451, -0.5608],\n",
      "          [-0.6863, -0.6706, -0.6392,  ..., -0.5294, -0.5373, -0.5451],\n",
      "          [-0.6706, -0.6706, -0.6392,  ..., -0.5373, -0.5373, -0.6157],\n",
      "          ...,\n",
      "          [-0.6000, -0.5843, -0.5451,  ..., -0.3882, -0.4824, -0.4745],\n",
      "          [-0.6078, -0.6000, -0.5529,  ..., -0.3725, -0.3647, -0.3647],\n",
      "          [-0.6314, -0.6157, -0.5686,  ..., -0.3176, -0.3333, -0.3569]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4902,  0.4824,  0.4275,  ..., -0.6549, -0.6627, -0.6627],\n",
      "          [ 0.3020,  0.1686,  0.0824,  ..., -0.6627, -0.6627, -0.6706],\n",
      "          [ 0.1373,  0.2863,  0.7098,  ..., -0.6549, -0.6549, -0.6627],\n",
      "          ...,\n",
      "          [ 0.3020,  0.3490,  0.5843,  ...,  0.7176,  0.7961,  0.7882],\n",
      "          [ 0.7098,  0.6941,  0.7490,  ...,  0.8039,  0.7647,  0.7490],\n",
      "          [ 0.6706,  0.7176,  0.7569,  ...,  0.8039,  0.7961,  0.7647]],\n",
      "\n",
      "         [[ 0.5843,  0.6314,  0.4980,  ..., -0.4588, -0.4667, -0.4667],\n",
      "          [ 0.3098,  0.1765, -0.0353,  ..., -0.4667, -0.4667, -0.4667],\n",
      "          [ 0.0353,  0.1294,  0.4118,  ..., -0.4588, -0.4588, -0.4745],\n",
      "          ...,\n",
      "          [ 0.2000,  0.2392,  0.4588,  ...,  0.6157,  0.6941,  0.6706],\n",
      "          [ 0.5608,  0.5451,  0.6157,  ...,  0.6549,  0.6549,  0.6314],\n",
      "          [ 0.5216,  0.5765,  0.6157,  ...,  0.6549,  0.6627,  0.6392]],\n",
      "\n",
      "         [[ 0.6392,  0.6471,  0.5294,  ..., -0.1373, -0.1451, -0.1765],\n",
      "          [ 0.3098,  0.1608, -0.0196,  ..., -0.1451, -0.1451, -0.1529],\n",
      "          [ 0.0039,  0.0980,  0.4039,  ..., -0.1373, -0.1373, -0.1529],\n",
      "          ...,\n",
      "          [ 0.2314,  0.2314,  0.4118,  ...,  0.5765,  0.6392,  0.6235],\n",
      "          [ 0.5294,  0.5059,  0.5608,  ...,  0.6000,  0.5765,  0.5608],\n",
      "          [ 0.4588,  0.5137,  0.5529,  ...,  0.6078,  0.6000,  0.5843]]]],\n",
      "       device='cuda:0'), None)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/CIFAR10/nootebooks/0.2-test-module.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/CIFAR10/nootebooks/0.2-test-module.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train()\n",
      "File \u001b[0;32m/CIFAR10/CIFAR_CNN/models/train_model.py:65\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch, device)\u001b[0m\n\u001b[1;32m     56\u001b[0m trainer \u001b[39m=\u001b[39m L\u001b[39m.\u001b[39mTrainer(\n\u001b[1;32m     57\u001b[0m     accelerator\u001b[39m=\u001b[39mACCELERATOR,\n\u001b[1;32m     58\u001b[0m     devices\u001b[39m=\u001b[39mdevice,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m     callbacks\u001b[39m=\u001b[39m callbacks\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     63\u001b[0m wandb\u001b[39m.\u001b[39mrun\n\u001b[0;32m---> 65\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model_module, datamodule \u001b[39m=\u001b[39;49m data_module)           \n\u001b[1;32m     66\u001b[0m trainer\u001b[39m.\u001b[39mtest(model_module,datamodule \u001b[39m=\u001b[39m data_module)\n\u001b[1;32m     68\u001b[0m wandb\u001b[39m.\u001b[39mfinish(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:545\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[1;32m    544\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    546\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    547\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:581\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    575\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    576\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    577\u001b[0m     ckpt_path,\n\u001b[1;32m    578\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    580\u001b[0m )\n\u001b[0;32m--> 581\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    583\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    584\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:990\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    987\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 990\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    992\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    994\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    995\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1036\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1034\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1035\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1036\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1037\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnexpected state \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:203\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start()\n\u001b[1;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance()\n\u001b[0;32m--> 203\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_advance_end()\n\u001b[1;32m    204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:372\u001b[0m, in \u001b[0;36m_FitLoop.on_advance_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    368\u001b[0m \u001b[39m# call train epoch end hooks\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[39m# we always call callback hooks first, but here we need to make an exception for the callbacks that\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[39m# monitor a metric, otherwise they wouldn't be able to monitor a key logged in\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[39m# `LightningModule.on_train_epoch_end`\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m call\u001b[39m.\u001b[39;49m_call_callback_hooks(trainer, \u001b[39m\"\u001b[39;49m\u001b[39mon_train_epoch_end\u001b[39;49m\u001b[39m\"\u001b[39;49m, monitoring_callbacks\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    373\u001b[0m call\u001b[39m.\u001b[39m_call_lightning_module_hook(trainer, \u001b[39m\"\u001b[39m\u001b[39mon_train_epoch_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    374\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(trainer, \u001b[39m\"\u001b[39m\u001b[39mon_train_epoch_end\u001b[39m\u001b[39m\"\u001b[39m, monitoring_callbacks\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:208\u001b[0m, in \u001b[0;36m_call_callback_hooks\u001b[0;34m(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(fn):\n\u001b[1;32m    207\u001b[0m         \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Callback]\u001b[39m\u001b[39m{\u001b[39;00mcallback\u001b[39m.\u001b[39mstate_key\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 208\u001b[0m             fn(trainer, trainer\u001b[39m.\u001b[39;49mlightning_module, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    210\u001b[0m \u001b[39mif\u001b[39;00m pl_module:\n\u001b[1;32m    211\u001b[0m     \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/CIFAR10/CIFAR_CNN/models/train_model.py:34\u001b[0m, in \u001b[0;36mImageCallback.on_train_epoch_end\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutputs)\n\u001b[1;32m     32\u001b[0m \u001b[39mprint\u001b[39m((x, y))\n\u001b[1;32m     33\u001b[0m captions \u001b[39m=\u001b[39m [\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mGround Truth: \u001b[39m\u001b[39m{\u001b[39;00my_i\u001b[39m}\u001b[39;00m\u001b[39m - Prediction: \u001b[39m\u001b[39m{\u001b[39;00my_pred\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m \n\u001b[0;32m---> 34\u001b[0m     \u001b[39mfor\u001b[39;00m y_i, y_pred \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(y[:n], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutputs[:n])]\n\u001b[1;32m     36\u001b[0m trainer\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mlog_image(\n\u001b[1;32m     37\u001b[0m         key\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msample_images\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m     38\u001b[0m         images\u001b[39m=\u001b[39mimages, \n\u001b[1;32m     39\u001b[0m         caption\u001b[39m=\u001b[39mcaptions)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/dl_env/lib/python3.10/site-packages/lightning/pytorch/loggers/wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainerFn.FITTING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type       | Params\n",
      "---------------------------------------------\n",
      "0 | cnn_relu_seq  | Sequential | 3.6 K \n",
      "1 | lin_layer_seq | Sequential | 93.4 K\n",
      "---------------------------------------------\n",
      "97.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "97.0 K    Total params\n",
      "0.388     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/dl_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  1.57it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/dl_env/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/dl_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  19%|█▉        | 33/176 [00:01<00:05, 27.87it/s, v_num=c9gq, val_loss=2.300, val_acc=0.103] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/dl_env/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "TrainerFn.TESTING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/root/miniconda3/envs/dl_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 40/40 [00:01<00:00, 24.78it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "          _acc              0.10000000149011612\n",
      "        test_loss           2.3018155097961426\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>lr-SGD</td><td>▁▁▂▃▅▆█</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▂▃▄▄▅▆▇▇█</td></tr><tr><td>val_acc</td><td>▁▁</td></tr><tr><td>val_loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>lr-SGD</td><td>0.00081</td></tr><tr><td>test_loss</td><td>2.30182</td></tr><tr><td>trainer/global_step</td><td>385</td></tr><tr><td>val_acc</td><td>0.1034</td></tr><tr><td>val_loss</td><td>2.30288</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">breezy-cherry-52</strong> at: <a href='https://wandb.ai/antzot/lightning_logs/runs/3zolc9gq' target=\"_blank\">https://wandb.ai/antzot/lightning_logs/runs/3zolc9gq</a><br/>Synced 6 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231029_014326-3zolc9gq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cifar_model = CIFAR10Model(num_classes=10,lr=2e-3)\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "wandb.run\n",
    "try:\n",
    "    wn_db = WandbLogger(log_model=True)\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        accelerator=\"gpu\",\n",
    "        devices=\"auto\",\n",
    "        max_epochs=45,\n",
    "        logger=wn_db,\n",
    "        callbacks=[lr_monitor]\n",
    "    )\n",
    "\n",
    "    trainer.fit(cifar_model,datamodule= load_module)\n",
    "\n",
    "    trainer.test(cifar_model,load_module)\n",
    "    wandb.finish(0)\n",
    "except():\n",
    "    wandb.finish(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
