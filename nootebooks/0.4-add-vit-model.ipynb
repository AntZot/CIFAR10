{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import lightning\n",
    "from einops import rearrange\n",
    "from einops import repeat\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePathces(nn.Module):\n",
    "    def __init__(self, img_size, in_ch:int = 3,patch_size: int = 16):\n",
    "        super().__init__()\n",
    "        assert img_size % patch_size == 0\n",
    "\n",
    "        emb_dim = patch_size * patch_size * in_ch\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,emb_dim))\n",
    "        self.pos_emb = nn.Parameter(torch.randn(1,1+(img_size//patch_size)**2,emb_dim))\n",
    "        self.patches = nn.Conv2d(in_ch,emb_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patches(x)\n",
    "\n",
    "        # x = torch.flatten(x,2)\n",
    "        # b, e, hw = x.shape\n",
    "        # x = x.reshape((b,hw,e))\n",
    "        x = rearrange(x, \"b e h w -> b (h w) e\")\n",
    "        \n",
    "        b, _, _ = x.shape\n",
    "\n",
    "        #cls_token = self.cls_token.repeat((b,1,1))\n",
    "        cls_token = repeat(self.cls_token,\"() s e -> b s e\",b=b)\n",
    "\n",
    "        x = torch.concat((x, cls_token),dim=1)\n",
    "\n",
    "        return self.pos_emb + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 197, 768])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch = ImagePathces(224,3)\n",
    "x = torch.randn(2, 3, 224, 224)\n",
    "patches = patch(x)\n",
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,input_size: int, hidden_size: int = None, output_size: int = None, droput = 0.,layer_act: torch.nn = nn.GELU) -> None:\n",
    "        super().__init__()\n",
    "        if hidden_size is None:\n",
    "            hidden_size = input_size\n",
    "        if output_size is None:\n",
    "            output_size = input_size\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size,hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size,output_size)\n",
    "        self.actv = layer_act()\n",
    "        self.drop = nn.Dropout(droput)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.drop(self.fc2(self.actv(self.fc1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLP(234,345,10,droput=0.2,layer_act=nn.ReLU)\n",
    "\n",
    "x = torch.randn((2,234))\n",
    "\n",
    "mlp(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self,dim: int,qkv_bias: bool = False, num_heads: int =8, attn_drop=0., out_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim,dim*3,bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.out_drop = nn.Dropout(out_drop)\n",
    "        self.out_lin = nn.Linear(dim,dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.attn_drop(self.qkv(x))\n",
    "        b, e, _ = x.shape\n",
    "        # b e (q k v)\n",
    "        x = x.reshape((b,3,self.num_heads,e,-1))\n",
    "        # b qkv heads e s \n",
    "        \n",
    "        att = F.log_softmax((x[:,0] @ x[:,1].transpose(2,3))*self.scale,dim=3)\n",
    "\n",
    "        att = att @ x[:,2]\n",
    "        \n",
    "        #out = out.reshape(b,e,-1)\n",
    "        att = rearrange(att, \"b h n e -> b n (h e)\")\n",
    "\n",
    "        out = self.out_drop(self.out_lin(att))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 197, 768])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att = Attention(768)\n",
    "att(patches).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, qkv_bias: bool = False, num_heads:int = 8, mlp_ratio:int = 4, drop_rate: float = 0.) -> None:\n",
    "        super().__init__()\n",
    "        self.att = Attention(dim,num_heads=num_heads,qkv_bias=qkv_bias)\n",
    "\n",
    "        self.att_norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.mlp = MLP(dim,dim*mlp_ratio,dim)\n",
    "        \n",
    "        self.mlp_norm = nn.LayerNorm(dim)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        att = self.att(x)\n",
    "        att_out = self.att_norm(x + att)\n",
    "\n",
    "        mlp = self.mlp(att_out)\n",
    "        mlp_out = self.mlp_norm(x + mlp)\n",
    "        return mlp_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 197, 768])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bl = Block(768)\n",
    "\n",
    "bl(patches).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim, qkv_bias: bool = False, N:int = 6, num_heads: int = 8, mlp_ratio: int = 4, drop_rate: float = 0.) -> None:\n",
    "        super().__init__()\n",
    "        self.blocks =nn.ModuleList([\n",
    "            Block(dim,qkv_bias,num_heads,mlp_ratio,drop_rate)\n",
    "        for _ in range(N)\n",
    "        ])\n",
    "\n",
    "    def forward(self,x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 197, 768])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = Encoder(768)\n",
    "\n",
    "enc(patches).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, img_size: int, patch_size: int, num_classes: int, \n",
    "                 in_ch: int = 3, depth: int=6, num_heads: int = 8, \n",
    "                 mlp_ratio: int = 4, qkv_bias=False, drop_rate: float = 0.) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        emb_dim = in_ch*patch_size**2\n",
    "        \n",
    "        self.patch = ImagePathces(img_size = img_size,\n",
    "                                  in_ch = in_ch,\n",
    "                                  patch_size = patch_size)\n",
    "\n",
    "        self.encoder = Encoder(dim = emb_dim,\n",
    "                               qkv_bias = qkv_bias,\n",
    "                               N = depth,\n",
    "                               num_heads = num_heads,\n",
    "                               mlp_ratio = mlp_ratio,\n",
    "                               drop_rate = drop_rate)\n",
    "        \n",
    "        self.mlp_head = nn.Linear(emb_dim,num_classes)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.patch(x)\n",
    "\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        x = self.mlp_head(x)\n",
    "        return F.softmax(x,dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 224, 224])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = ViT(img_size=224,\n",
    "          patch_size=16,\n",
    "          num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "from cifar_cnn.data.data_loader import CIFAR10DataModule\n",
    "#from cifar_cnn.models.cnn import CIFAR10Model\n",
    "from cifar_cnn.models.vit import ViT\n",
    "import torch\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "import wandb\n",
    "\n",
    "ACCELERATOR = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "CONF = {\n",
    "    \"architecture\": \"ViT\", \n",
    "    \"batch_size\": 20\n",
    "    }\n",
    "\n",
    "\n",
    "class ImageCallback(L.Callback):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.outputs = None\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "\n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        if batch_idx == trainer.num_training_batches-1:\n",
    "            self.x, self.y = batch\n",
    "            self.outputs = torch.argmax(outputs[\"prediction\"],dim=1)\n",
    "\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        n = 10\n",
    "        x, y = self.x, self.y\n",
    "\n",
    "        images = [img for img in x[:n]]\n",
    "        captions = [f'Target: {y_i} - Prediction: {y_pred}' \n",
    "            for y_i, y_pred in zip(y[:n], self.outputs[:n])]\n",
    "\n",
    "        trainer.logger.log_image(\n",
    "                key='sample_images', \n",
    "                images=images, \n",
    "                caption=captions)\n",
    "\n",
    "\n",
    "callbacks =[\n",
    "    LearningRateMonitor(logging_interval='step'),\n",
    "    ImageCallback()  \n",
    "]\n",
    "\n",
    "   \n",
    "def train(epoch: int = 10,\n",
    "          device: str = \"auto\",\n",
    "          lr: float = 2e-3,\n",
    "          path: str  = \"/CIFAR10/datasets/raw\") -> None:\n",
    "\n",
    "    data_module = CIFAR10DataModule(path)\n",
    "    model_module = ViT(\n",
    "        img_size=32,\n",
    "        patch_size=8,\n",
    "        num_classes=10,\n",
    "        in_ch=3,\n",
    "        num_heads= 2\n",
    "    )\n",
    "    \n",
    "    LOGGER = WandbLogger(log_model=True, name=f\"{CONF['architecture']}-lr({lr})-epoch({epoch})\")\n",
    "    try:\n",
    "        trainer = L.Trainer(\n",
    "            accelerator=ACCELERATOR,\n",
    "            devices=device,\n",
    "            max_epochs=epoch,\n",
    "            callbacks= callbacks\n",
    "        )\n",
    "        wandb.run\n",
    "\n",
    "        trainer.fit(model_module, datamodule = data_module)           \n",
    "        trainer.test(model_module,datamodule = data_module)\n",
    "\n",
    "        wandb.finish(0)\n",
    "    except RuntimeError:\n",
    "        wandb.finish(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/root/miniconda3/envs/dl_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "TrainerFn.FITTING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type         | Params\n",
      "------------------------------------------\n",
      "0 | patch    | ImagePathces | 40.5 K\n",
      "1 | encoder  | Encoder      | 2.7 M \n",
      "2 | mlp_head | Linear       | 1.9 K \n",
      "------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.833    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "train(epoch=8,lr=1e-5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
