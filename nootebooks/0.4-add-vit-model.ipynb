{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import lightning\n",
    "from einops import rearrange\n",
    "from einops import repeat\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePathces(nn.Module):\n",
    "    def __init__(self, img_size, in_ch:int = 3,patch_size: int = 16):\n",
    "        super().__init__()\n",
    "        assert img_size % patch_size == 0\n",
    "\n",
    "        emb_dim = patch_size * patch_size * in_ch\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,emb_dim))\n",
    "\n",
    "        self.pos_emb = nn.Parameter(torch.randn(1,1+(img_size//patch_size)**2,emb_dim))\n",
    "        \n",
    "        self.patches = nn.Conv2d(in_ch,emb_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patches(x)\n",
    "\n",
    "        # x = torch.flatten(x,2)\n",
    "        # b, e, hw = x.shape\n",
    "        # x = x.reshape((b,hw,e))\n",
    "        x = rearrange(x, \"b e h w -> b (h w) e\")\n",
    "        \n",
    "        b, _, _ = x.shape\n",
    "\n",
    "        #cls_token = self.cls_token.repeat((b,1,1))\n",
    "        cls_token = repeat(self.cls_token,\"() s e -> b s e\",b=b)\n",
    "\n",
    "        x = torch.concat((cls_token, x),dim=1)\n",
    "\n",
    "        return x + self.pos_emb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4626, -0.0470,  0.2544,  ..., -2.6658, -1.4862, -0.7106],\n",
       "         [-2.3378,  0.1920,  0.3672,  ..., -2.1694,  1.0998, -0.4936],\n",
       "         [ 0.6476, -0.1898, -1.6607,  ...,  0.6751, -2.0205,  1.0907],\n",
       "         ...,\n",
       "         [-1.5856, -0.0727, -1.5754,  ...,  1.2539,  0.0736,  1.1453],\n",
       "         [-0.0046,  0.9536, -1.8399,  ..., -0.7205, -0.2709, -0.6958],\n",
       "         [ 0.5433, -2.4332,  1.8318,  ..., -1.4578, -1.5394, -0.6127]],\n",
       "\n",
       "        [[ 0.4626, -0.0470,  0.2544,  ..., -2.6658, -1.4862, -0.7106],\n",
       "         [-2.1221,  0.9999,  0.6679,  ..., -2.4711,  0.7421, -0.1767],\n",
       "         [-0.3135, -0.4021, -1.0341,  ...,  0.2260, -1.9319,  1.4878],\n",
       "         ...,\n",
       "         [-1.9408, -0.1953, -0.7337,  ...,  0.5273,  0.5717,  1.5941],\n",
       "         [ 0.2360,  0.7556, -1.9966,  ...,  0.2253,  0.8419, -1.2392],\n",
       "         [ 0.6351, -2.9221,  1.7094,  ..., -2.7709, -0.1879, -1.3557]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch = ImagePathces(224,3)\n",
    "x = torch.randn(2, 3, 224, 224)\n",
    "patches = patch(x)\n",
    "patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,input_size: int, hidden_size: int = None, output_size: int = None, droput = 0.,layer_act: torch.nn = nn.GELU) -> None:\n",
    "        super().__init__()\n",
    "        if hidden_size is None:\n",
    "            hidden_size = input_size\n",
    "        if output_size is None:\n",
    "            output_size = input_size\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size,hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size,output_size)\n",
    "        self.actv = layer_act()\n",
    "        self.drop = nn.Dropout(droput)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.drop(self.fc2(self.actv(self.fc1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLP(234,345,10,droput=0.2,layer_act=nn.ReLU)\n",
    "\n",
    "x = torch.randn((2,234))\n",
    "\n",
    "mlp(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self,dim: int,qkv_bias: bool = False, num_heads: int =8, attn_drop=0., out_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim,dim*3,bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.out_drop = nn.Dropout(out_drop)\n",
    "        self.out_lin = nn.Linear(dim,dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.qkv(x)\n",
    "        b, e, _ = x.shape\n",
    "        # b e (q k v)\n",
    "        x = x.reshape((b,3,self.num_heads,e,-1))\n",
    "        # b qkv heads e s \n",
    "        \n",
    "        att = F.softmax((x[:,0] @ x[:,1].transpose(2,3))*self.scale, dim=2)\n",
    "\n",
    "        att = self.attn_drop(att @ x[:,2])\n",
    "        \n",
    "        #out = out.reshape(b,e,-1)\n",
    "        att = rearrange(att, \"b h n e -> b n (h e)\")\n",
    "\n",
    "        out = self.out_drop(self.out_lin(att))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 197, 768])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att = Attention(768)\n",
    "att(patches).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, qkv_bias: bool = False, num_heads:int = 8, mlp_ratio:int = 4, drop_rate: float = 0.) -> None:\n",
    "        super().__init__()\n",
    "        self.att = Attention(dim,num_heads=num_heads,qkv_bias=qkv_bias)\n",
    "\n",
    "        self.att_norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.mlp = MLP(dim,dim*mlp_ratio,dim)\n",
    "        \n",
    "        self.mlp_norm = nn.LayerNorm(dim)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        att = self.att(x)\n",
    "        att_out = self.att_norm(x + att)\n",
    "\n",
    "        mlp = self.mlp(att_out)\n",
    "        mlp_out = self.mlp_norm(x + mlp)\n",
    "        return mlp_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 197, 768])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bl = Block(768)\n",
    "\n",
    "bl(patches).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim, qkv_bias: bool = False, N:int = 6, num_heads: int = 8, mlp_ratio: int = 4, drop_rate: float = 0.) -> None:\n",
    "        super().__init__()\n",
    "        self.blocks =nn.ModuleList([\n",
    "            Block(dim,qkv_bias,num_heads,mlp_ratio,drop_rate)\n",
    "        for _ in range(N)\n",
    "        ])\n",
    "\n",
    "    def forward(self,x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 197, 768])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = Encoder(768)\n",
    "\n",
    "enc(patches).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, img_size: int, patch_size: int, num_classes: int, \n",
    "                 in_ch: int = 3, depth: int=6, num_heads: int = 8, \n",
    "                 mlp_ratio: int = 4, qkv_bias=False, drop_rate: float = 0.) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        emb_dim = in_ch*patch_size**2\n",
    "        \n",
    "        self.patch = ImagePathces(img_size = img_size,\n",
    "                                  in_ch = in_ch,\n",
    "                                  patch_size = patch_size)\n",
    "\n",
    "        self.encoder = Encoder(dim = emb_dim,\n",
    "                               qkv_bias = qkv_bias,\n",
    "                               N = depth,\n",
    "                               num_heads = num_heads,\n",
    "                               mlp_ratio = mlp_ratio,\n",
    "                               drop_rate = drop_rate)\n",
    "        \n",
    "        self.mlp_head = nn.Linear(emb_dim,num_classes)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.patch(x)\n",
    "\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        x = self.mlp_head(x)\n",
    "        return F.softmax(x[:,0],dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = ViT(img_size=224,\n",
    "          patch_size=16,\n",
    "          num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning as L\n",
    "from typing import Any, Tuple\n",
    "from torchmetrics.functional import accuracy, auroc\n",
    "from cifar_cnn.models.SKConv import SKConv\n",
    "from cifar_cnn.models.vit import ViT\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class Model(L.LightningModule):\n",
    "    def __init__(self, img_size: int, patch_size: int, num_classes: int, \n",
    "                 in_ch: int = 3, depth: int=6, num_heads: int = 8, \n",
    "                 mlp_ratio: int = 4, qkv_bias=False, drop_rate: float = 0.,\n",
    "                 lr=1e-5,in_channels:int = 3,out_channels: int = 4,) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate = lr\n",
    "\n",
    "        self.skconv = SKConv(in_channels,out_channels,groups=1)\n",
    "\n",
    "        self.ViT =  ViT(img_size=img_size,\n",
    "                        patch_size=patch_size,\n",
    "                        num_classes=num_classes,\n",
    "                        in_ch=in_ch,\n",
    "                        depth=depth,\n",
    "                        num_heads=num_heads,\n",
    "                        mlp_ratio=mlp_ratio,\n",
    "                        drop_rate=drop_rate)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.skconv(x)\n",
    "        x = self.ViT(x)\n",
    "        return x\n",
    "\n",
    "        \n",
    "    def training_step(self, batch, batch_nb: int):\n",
    "        x,y = batch\n",
    "\n",
    "        preds = self(x)\n",
    "        loss = F.cross_entropy(preds,y)\n",
    "        \n",
    "        return {'loss': loss, 'prediction': preds}\n",
    "    \n",
    "    def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \"\"\"\n",
    "        metrics\n",
    "        \"\"\"\n",
    "        rocauc = auroc(logits,y,task=\"multiclass\",num_classes=self.num_classes)\n",
    "        acc = accuracy(logits, y, task=\"multiclass\",num_classes=self.num_classes)\n",
    "        self.log(\"val_accuracy\", acc, prog_bar=True)\n",
    "        self.log(\"val_rocauc\",rocauc,prog_bar=True)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "\n",
    "    def test_step(self,batch: Tuple[torch.Tensor, torch.Tensor], batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        \n",
    "        test_loss = F.cross_entropy(logits, y)\n",
    "        \"\"\"\n",
    "        metrics\n",
    "        \"\"\"\n",
    "        rocauc = auroc(logits,y,task=\"multiclass\",num_classes=self.num_classes)\n",
    "        acc = accuracy(logits, y, task=\"multiclass\",num_classes=self.num_classes)\n",
    "        self.log(\"test_accuracy\", acc, prog_bar=True)\n",
    "        self.log(\"test_rocauc\",rocauc,prog_bar=True)\n",
    "        self.log(\"test_loss\", test_loss, prog_bar=True)\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), self.learning_rate)\n",
    "\n",
    "        scheduler = {\n",
    "            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer=optimizer,patience=5),\n",
    "            \"monitor\": \"val_loss\"\n",
    "        }\n",
    "        \n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\":scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import STL10, CIFAR10\n",
    "from torchvision.transforms import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5)) \n",
    "            ]\n",
    "        )\n",
    "dims = (3,32,32)\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = STL10(\"/CIFAR10/datasets/raw\",split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import EuroSAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://madm.dfki.de/files/sentinel/EuroSAT.zip to /CIFAR10/datasets/raw/eurosat/EuroSAT.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94280567/94280567 [00:08<00:00, 11104301.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /CIFAR10/datasets/raw/eurosat/EuroSAT.zip to /CIFAR10/datasets/raw/eurosat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset EuroSAT\n",
       "    Number of datapoints: 27000\n",
       "    Root location: /CIFAR10/datasets/raw"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EuroSAT(\"/CIFAR10/datasets/raw\",download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "from cifar_cnn.data.data_loader import CIFAR10DataModule, STL10DataModule\n",
    "#from cifar_cnn.models.cnn import CIFAR10Model\n",
    "from cifar_cnn.models.vit import ViT\n",
    "import torch\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint, EarlyStopping\n",
    "import wandb\n",
    "\n",
    "ACCELERATOR = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "CONF = {\n",
    "    \"architecture\": \"ViT\", \n",
    "    \"batch_size\": 64,\n",
    "    \"dataset\" : \"CIFAR10\"\n",
    "    }\n",
    "PARAMS = {\n",
    "        \"lr\":0.0001,\n",
    "        # \"in_channels\":3,\n",
    "        # \"out_channels\":4,\n",
    "        \"img_size\":32,\n",
    "        \"patch_size\":4,\n",
    "        \"num_classes\":10,\n",
    "        \"in_ch\":3,\n",
    "        \"mlp_ratio\": 4,\n",
    "        \"depth\":12,\n",
    "        \"num_heads\":8,\n",
    "        \"drop_rate\":0.\n",
    "    }\n",
    "\n",
    "\n",
    "class ImageCallback(L.Callback):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.outputs = None\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "\n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        if batch_idx == trainer.num_training_batches-1:\n",
    "            self.x, self.y = batch\n",
    "            self.outputs = torch.argmax(outputs[\"prediction\"],dim=1)\n",
    "\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        n = 10\n",
    "        x, y = self.x, self.y\n",
    "\n",
    "        images = [img for img in x[:n]]\n",
    "        captions = [f'Target: {y_i} - Prediction: {y_pred}' \n",
    "            for y_i, y_pred in zip(y[:n], self.outputs[:n])]\n",
    "\n",
    "        trainer.logger.log_image(\n",
    "                key='sample_images', \n",
    "                images=images, \n",
    "                caption=captions)\n",
    "\n",
    "\n",
    "callbacks =[\n",
    "    LearningRateMonitor(logging_interval='step'),\n",
    "    ImageCallback(),\n",
    "    EarlyStopping('val_loss',patience=50)  \n",
    "]\n",
    "\n",
    "   \n",
    "def train(epoch: int = 10,\n",
    "          device: str = \"auto\",\n",
    "          path: str  = \"/CIFAR10/datasets/raw\",\n",
    "          save_rate: int = 0.1) -> None:\n",
    "\n",
    "    callbacks.append(ModelCheckpoint(dirpath='CIFAR10/checkpoints/',\n",
    "                                     filename=f\"epoch({epoch})-{CONF['dataset']}\",every_n_epochs=epoch//(save_rate*100)))\n",
    "\n",
    "    conf = CONF | PARAMS\n",
    "\n",
    "    LOGGER = WandbLogger(log_model=True, name=f\"{CONF['architecture']}-{CONF['dataset']}-lr({PARAMS['lr']})-epoch({epoch})\")\n",
    "    LOGGER.experiment.config.update(conf)\n",
    "\n",
    "    data_module = CIFAR10DataModule(path)\n",
    "\n",
    "    model_module = ViT(**PARAMS)\n",
    "    model_module._set_batch_and_dt_size(data_module.batch_size,data_module.size)\n",
    "\n",
    "    try:\n",
    "        trainer = L.Trainer(\n",
    "            accelerator=ACCELERATOR,\n",
    "            devices=device,\n",
    "            max_epochs=epoch,\n",
    "            logger=LOGGER,\n",
    "            callbacks= callbacks\n",
    "        )\n",
    "        wandb.run\n",
    "\n",
    "        trainer.fit(model_module, datamodule = data_module)           \n",
    "        trainer.test(model_module,datamodule = data_module)\n",
    "\n",
    "\n",
    "        wandb.finish(0)\n",
    "    except RuntimeError as err:\n",
    "        wandb.finish(1)\n",
    "        print(err.with_traceback())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mantzot\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20231123_135549-sh0ugmdf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/antzot/lightning_logs/runs/sh0ugmdf' target=\"_blank\">ViT-CIFAR10-lr(1e-05)-epoch(50)</a></strong> to <a href='https://wandb.ai/antzot/lightning_logs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/antzot/lightning_logs' target=\"_blank\">https://wandb.ai/antzot/lightning_logs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/antzot/lightning_logs/runs/sh0ugmdf' target=\"_blank\">https://wandb.ai/antzot/lightning_logs/runs/sh0ugmdf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 4060 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/root/miniconda3/envs/dl_env/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:630: Checkpoint directory CIFAR10/checkpoints/ exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type         | Params\n",
      "------------------------------------------\n",
      "0 | patch    | ImagePathces | 5.5 K \n",
      "1 | encoder  | Encoder      | 337 K \n",
      "2 | mlp_head | Sequential   | 586   \n",
      "------------------------------------------\n",
      "343 K     Trainable params\n",
      "0         Non-trainable params\n",
      "343 K     Total params\n",
      "1.375     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/dl_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/root/miniconda3/envs/dl_env/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/dl_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 704/704 [00:27<00:00, 25.43it/s, v_num=gmdf, train_accuracy=0.125, train_loss=2.300] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/dl_env/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:  19%|█▉        | 135/704 [00:05<00:21, 26.76it/s, v_num=gmdf, train_accuracy=0.234, train_loss=2.190, val_accuracy=0.284, val_rocauc=0.715, val_loss=2.190] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/dl_env/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/root/miniconda3/envs/dl_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 157/157 [00:04<00:00, 32.89it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test_accuracy         0.2948000133037567\n",
      "        test_loss            2.180657148361206\n",
      "       test_rocauc          0.7174748182296753\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    }
   ],
   "source": [
    "train(epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "from cifar_cnn.data.data_loader import CIFAR10DataModule, STL10DataModule\n",
    "#from cifar_cnn.models.cnn import CIFAR10Model\n",
    "from cifar_cnn.models.vit import ViT\n",
    "import torch\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint, EarlyStopping\n",
    "import wandb\n",
    "\n",
    "ACCELERATOR = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "CONF = {\n",
    "    \"architecture\": \"ViT\", \n",
    "    \"batch_size\": 64,\n",
    "    \"dataset\" : \"STL10->CIFAR10\"\n",
    "    }\n",
    "PARAMS = {\n",
    "        \"lr\":1e-5,\n",
    "        # \"in_channels\":3,\n",
    "        # \"out_channels\":4,\n",
    "        \"img_size\":32,\n",
    "        \"patch_size\":16,\n",
    "        \"num_classes\":10,\n",
    "        \"in_ch\":3,\n",
    "        \"mlp_ratio\": 6,\n",
    "        \"depth\":20,\n",
    "        \"num_heads\":8,\n",
    "        \"drop_rate\":0.\n",
    "    }\n",
    "\n",
    "\n",
    "class ImageCallback(L.Callback):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.outputs = None\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "\n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        if batch_idx == trainer.num_training_batches-1:\n",
    "            self.x, self.y = batch\n",
    "            self.outputs = torch.argmax(outputs[\"prediction\"],dim=1)\n",
    "\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        n = 10\n",
    "        x, y = self.x, self.y\n",
    "\n",
    "        images = [img for img in x[:n]]\n",
    "        captions = [f'Target: {y_i} - Prediction: {y_pred}' \n",
    "            for y_i, y_pred in zip(y[:n], self.outputs[:n])]\n",
    "\n",
    "        trainer.logger.log_image(\n",
    "                key='sample_images', \n",
    "                images=images, \n",
    "                caption=captions)\n",
    "\n",
    "\n",
    "callbacks =[\n",
    "    LearningRateMonitor(logging_interval='step'),\n",
    "    ImageCallback(),\n",
    "    EarlyStopping('val_loss',patience=50)  \n",
    "]\n",
    "\n",
    "   \n",
    "def sec_train(epoch: int = 10,\n",
    "          device: str = \"auto\",\n",
    "          path: str  = \"/CIFAR10/datasets/raw\",\n",
    "          save_rate: int = 0.1) -> None:\n",
    "\n",
    "    callbacks.append(ModelCheckpoint(dirpath='CIFAR10/checkpoints/',\n",
    "                                     filename=f\"epoch({epoch})-{CONF['dataset']}\",every_n_epochs=epoch//(save_rate*100)))\n",
    "\n",
    "    conf = CONF | PARAMS\n",
    "\n",
    "    LOGGER = WandbLogger(log_model=True, name=f\"{CONF['architecture']}-{CONF['dataset']}-lr({PARAMS['lr']})-epoch({epoch})\")\n",
    "    LOGGER.experiment.config.update(conf)\n",
    "\n",
    "    data_module = CIFAR10DataModule(path)\n",
    "\n",
    "    model_module = ViT.load_from_checkpoint(fr'/CIFAR10/nootebooks/CIFAR10/epoch({epoch})-STL10-v1.ckpt',img_size=32)\n",
    "\n",
    "    model_module._set_batch_and_dt_size(data_module.batch_size,data_module.size)\n",
    "    try:\n",
    "        trainer = L.Trainer(\n",
    "            accelerator=ACCELERATOR,\n",
    "            devices=device,\n",
    "            max_epochs=epoch,\n",
    "            logger=LOGGER,\n",
    "            callbacks= callbacks)\n",
    "        \n",
    "        trainer.fit(model_module, datamodule = data_module)           \n",
    "        trainer.test(model_module,datamodule = data_module)\n",
    "\n",
    "        wandb.finish(0)\n",
    "    except RuntimeError as err:\n",
    "        wandb.finish(1)\n",
    "        print(err.with_traceback())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
