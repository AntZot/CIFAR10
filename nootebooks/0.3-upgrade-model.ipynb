{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "from typing import List, Optional\n",
    "\n",
    "from climex.data.data_loader import CIFAR10DataModule\n",
    "\n",
    "from climex.models.ResSKBlock import ResSKBlock\n",
    "from climex.models.SKConv import SKConv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CIFAR10DataModule(\"/CIFAR10/datasets/raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "data.prepare_data()\n",
    "data.setup()\n",
    "data = data.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = data.dataset.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 3, 32, 32])\n",
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "for i,j in data:\n",
    "    print(i.shape)\n",
    "    print(j.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data shapes is 255 3 32 32 (b,c,H,H) and 50000 3 32 32 in original dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SK Res Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cifar_cnn.models.ResSKBlock import ResSKBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from torch import nn\n",
    "import torch\n",
    "import lightning as L\n",
    "from torchmetrics.functional import accuracy, auroc\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256 if torch.cuda.is_available() else 64\n",
    "\n",
    "class CIFAR10Model(L.LightningModule):\n",
    "\n",
    "    def __init__(self,num_classes,lr):\n",
    "        super().__init__()\n",
    "        self.learning_rate = lr\n",
    "        self.num_classes = num_classes\n",
    "        self.cnn_relu_seq = nn.Sequential(\n",
    "            nn.Conv2d(3,16,5),\n",
    "            nn.ReLU(),\n",
    "            ResSKBlock(in_channels=16,out_channels=16*2,groups = 4),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.lin_layer_seq = nn.Sequential(\n",
    "            nn.Linear(32*28*28,12544),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12544,6272),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6272,784),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(784,10)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        conv_res = self.cnn_relu_seq(x)\n",
    "        flattened = conv_res.view(conv_res.size(0), -1)\n",
    "        lin_res = self.lin_layer_seq(flattened)\n",
    "        return F.log_softmax(lin_res,dim=1)\n",
    "\n",
    "    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_nb: int) -> torch.Tensor:\n",
    "        x,y = batch\n",
    "        loss = F.cross_entropy(self(x), y)\n",
    "        preds = self(x)\n",
    "\n",
    "        \"\"\"\n",
    "        metrics\n",
    "        \"\"\"\n",
    "        rocauc = auroc(preds, y, task=\"multiclass\",num_classes=self.num_classes)\n",
    "        self.log(\"train_rocauc\", rocauc, prog_bar=True)\n",
    "        acc = accuracy(preds, y, task=\"multiclass\",num_classes=self.num_classes)\n",
    "        self.log(\"train_accuracy\", acc, prog_bar=True)\n",
    "        return {'loss': loss, 'prediction': preds}\n",
    "\n",
    "    def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \"\"\"\n",
    "        metrics\n",
    "        \"\"\"\n",
    "        rocauc = auroc(logits,y,task=\"multiclass\",num_classes=self.num_classes)\n",
    "        acc = accuracy(logits, y, task=\"multiclass\",num_classes=self.num_classes)\n",
    "        self.log(\"val_accuracy\", acc, prog_bar=True)\n",
    "        self.log(\"val_rocauc\",rocauc,prog_bar=True)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "\n",
    "    def test_step(self,batch: Tuple[torch.Tensor, torch.Tensor], batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "\n",
    "        test_loss = F.cross_entropy(logits, y)\n",
    "        \"\"\"\n",
    "        metrics\n",
    "        \"\"\"\n",
    "        rocauc = auroc(logits,y,task=\"multiclass\",num_classes=self.num_classes)\n",
    "        acc = accuracy(logits, y, task=\"multiclass\",num_classes=self.num_classes)\n",
    "        self.log(\"test_accuracy\", acc, prog_bar=True)\n",
    "        self.log(\"test_rocauc\",rocauc,prog_bar=True)\n",
    "        self.log(\"test_loss\", test_loss, prog_bar=True)\n",
    "        \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), self.learning_rate,momentum=0.9)\n",
    "        scheduler = {\n",
    "            \"scheduler\": torch.optim.lr_scheduler.OneCycleLR(\n",
    "                optimizer=optimizer,\n",
    "                max_lr=1e-2,\n",
    "                epochs=self.trainer.max_epochs,\n",
    "                steps_per_epoch = 50000 // BATCH_SIZE),\n",
    "            \"interval\": \"step\"\n",
    "        }\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\":  scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 4060 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainerFn.FITTING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type       | Params\n",
      "---------------------------------------------\n",
      "0 | cnn_relu_seq  | Sequential | 15.7 K\n",
      "1 | lin_layer_seq | Sequential | 398 M \n",
      "---------------------------------------------\n",
      "398 M     Trainable params\n",
      "0         Non-trainable params\n",
      "398 M     Total params\n",
      "1,593.361 Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/dl_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/root/miniconda3/envs/dl_env/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/dl_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 176/176 [00:44<00:00,  3.99it/s, v_num=6bty, train_rocauc=1.000, train_accuracy=0.985, val_accuracy=0.685, val_rocauc=0.948, val_loss=1.130]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=8` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 176/176 [00:55<00:00,  3.17it/s, v_num=6bty, train_rocauc=1.000, train_accuracy=0.985, val_accuracy=0.685, val_rocauc=0.948, val_loss=1.130]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "TrainerFn.TESTING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/root/miniconda3/envs/dl_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 40/40 [00:05<00:00,  6.97it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/dl_env/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test_accuracy         0.6758000254631042\n",
      "        test_loss            1.149863362312317\n",
      "       test_rocauc          0.9462881088256836\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇█</td></tr><tr><td>lr-SGD</td><td>▁▂▃▄▅▆▇██████▇▇▆▆▆▅▄▄▃▃▂▂▂▁▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>test_rocauc</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▁▂▃▃▄▄▄▅▄▅▅▅▆▆▅▆▆▆▆▇▇▇▇█▇███</td></tr><tr><td>train_rocauc</td><td>▁▃▄▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_accuracy</td><td>▁▄▅▆▆▆▇█</td></tr><tr><td>val_loss</td><td>█▄▂▁▁▃▂▁</td></tr><tr><td>val_rocauc</td><td>▁▅▆▇▇▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>8</td></tr><tr><td>lr-SGD</td><td>0.00052</td></tr><tr><td>test_accuracy</td><td>0.6758</td></tr><tr><td>test_loss</td><td>1.14986</td></tr><tr><td>test_rocauc</td><td>0.94629</td></tr><tr><td>train_accuracy</td><td>0.98438</td></tr><tr><td>train_rocauc</td><td>0.99982</td></tr><tr><td>trainer/global_step</td><td>1408</td></tr><tr><td>val_accuracy</td><td>0.6846</td></tr><tr><td>val_loss</td><td>1.12585</td></tr><tr><td>val_rocauc</td><td>0.94798</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">balmy-dream-94</strong> at: <a href='https://wandb.ai/antzot/lightning_logs/runs/m2416bty' target=\"_blank\">https://wandb.ai/antzot/lightning_logs/runs/m2416bty</a><br/>Synced 6 W&B file(s), 80 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231104_155846-m2416bty/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import List, Union\n",
    "from cifar_cnn.data.data_loader import CIFAR10DataModule\n",
    "import torch\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "import wandb\n",
    "\n",
    "ACCELERATOR = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "LOGGER = WandbLogger(log_model=True)\n",
    "LOGGER.experiment.config.update({\"architecture\": \"resnet\", \"batch_size\": 256})\n",
    "\n",
    "\n",
    "class ImageCallback(L.Callback):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.outputs = None\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "\n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        if batch_idx == trainer.num_training_batches-1:\n",
    "            self.x, self.y = batch\n",
    "            self.outputs = torch.argmax(outputs[\"prediction\"],dim=1)\n",
    "\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        n = 10\n",
    "        x, y = self.x, self.y\n",
    "\n",
    "        images = [img for img in x[:n]]\n",
    "        captions = [f'Target: {y_i} - Prediction: {y_pred}' \n",
    "            for y_i, y_pred in zip(y[:n], self.outputs[:n])]\n",
    "\n",
    "        trainer.logger.log_image(\n",
    "                key='sample_images', \n",
    "                images=images, \n",
    "                caption=captions)\n",
    "\n",
    "\n",
    "callbacks =[\n",
    "    LearningRateMonitor(logging_interval='step'),\n",
    "    ImageCallback()  \n",
    "]\n",
    "\n",
    "   \n",
    "def train(epoch: int = 45,\n",
    "          device: str = \"auto\",\n",
    "          lr: float = 2e-3,\n",
    "          path: str  = \"/CIFAR10/datasets/raw\") -> None:\n",
    "\n",
    "    data_module = CIFAR10DataModule(path)\n",
    "    model_module = CIFAR10Model(num_classes=data_module.num_classes,lr=lr)\n",
    "    try:\n",
    "        trainer = L.Trainer(\n",
    "            accelerator=ACCELERATOR,\n",
    "            devices=device,\n",
    "            max_epochs=epoch,\n",
    "            logger=LOGGER,\n",
    "            callbacks= callbacks\n",
    "        )\n",
    "\n",
    "        trainer.fit(model_module, datamodule = data_module)           \n",
    "        trainer.test(model_module,datamodule = data_module)\n",
    "\n",
    "        wandb.finish(0)\n",
    "    except RuntimeError:\n",
    "        wandb.finish(1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train(epoch=8,lr=1e-5)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
